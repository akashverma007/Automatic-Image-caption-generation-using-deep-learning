{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG16_with_LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VLtqgE3a-jt","executionInfo":{"status":"ok","timestamp":1633447223357,"user_tz":-330,"elapsed":31207,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuFp5nYuG4zEoEYyiN9Oc_5LtVBAEcUtSDRkzDtw=s64","userId":"06075092917726984214"}},"outputId":"5773d3d9-3534-472a-bea0-3d827e76e371"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"MzmV11TssCZz"},"source":["#Import Necessary Libraries\n","from os import listdir\n","from pickle import dump\n","from keras.applications.vgg16 import VGG16\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from keras.models import Model\n","\n","# extract features from each photo in the directory\n","def extract_features(directory):\n","\t# load the model\n","\tmodel = VGG16(include_top=True, weights='imagenet')\n","\t# re-structure the model\n","\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n","\t# summarize\n","\tprint(model.summary())\n","\t# extract features from each photo\n","\tfeatures = dict()\n","\tfor name in listdir(directory):\n","\t\t# load an image from file\n","\t\tfilename = directory + '/' + name\n","\t\timage = load_img(filename, target_size=(224, 224))\n","\t\t# convert the image pixels to a numpy array\n","\t\timage = img_to_array(image)\n","\t\t# reshape data for the model\n","\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","\t\t# prepare the image for the VGG model\n","\t\timage = preprocess_input(image)\n","\t\t# get features\n","\t\tfeature = model.predict(image, verbose=0)\n","\t\t# get image id\n","\t\timage_id = name.split('.')[0]\n","\t\t# store feature\n","\t\tfeatures[image_id] = feature\n","\t\tprint(len(features) ,':', name)\n","\treturn features\n","\n","# extract features from all images\n","directory = '/content/drive/MyDrive/ICG/Flickr8k/Flicker8k_Dataset'\n","features = extract_features(directory)\n","print('Extracted Features: %d' % len(features))\n","# save to file\n","dump(features, open('/content/drive/MyDrive/ICG/VGG16_with_LSTM/features.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RrM34MWy42hf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633447317232,"user_tz":-330,"elapsed":60545,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuFp5nYuG4zEoEYyiN9Oc_5LtVBAEcUtSDRkzDtw=s64","userId":"06075092917726984214"}},"outputId":"a2597161-1d37-4b75-8794-32a5b2db7509"},"source":["!pip install tensorflow==2.0.0\n","!pip install keras==2.3.1"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.0.0\n","  Downloading tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3 MB)\n","\u001b[K     |████████████████████████████████| 86.3 MB 42 kB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.19.5)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n","\u001b[K     |████████████████████████████████| 449 kB 42.2 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.15.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.3.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.8.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.2)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 34.7 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.40.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.37.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.12.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.17.3)\n","Collecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.12.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (3.1.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.3.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.35.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0.0) (1.5.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.5.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=e82fc40c24baac997c9e69d3c62619bdbe605f663ab7594dc9258d5c019ea06f\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.6.0\n","    Uninstalling tensorflow-estimator-2.6.0:\n","      Successfully uninstalled tensorflow-estimator-2.6.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.6.0\n","    Uninstalling tensorboard-2.6.0:\n","      Successfully uninstalled tensorboard-2.6.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.6.0\n","    Uninstalling tensorflow-2.6.0:\n","      Successfully uninstalled tensorflow-2.6.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n","Collecting keras==2.3.1\n","  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n","\u001b[K     |████████████████████████████████| 377 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.1.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.3.1) (1.5.2)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.6.0\n","    Uninstalling keras-2.6.0:\n","      Successfully uninstalled keras-2.6.0\n","Successfully installed keras-2.3.1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBA9SMdC2-KL","executionInfo":{"status":"ok","timestamp":1633447324763,"user_tz":-330,"elapsed":2215,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuFp5nYuG4zEoEYyiN9Oc_5LtVBAEcUtSDRkzDtw=s64","userId":"06075092917726984214"}},"outputId":"7ea4adc7-65af-4921-ba7c-db4983f4e287"},"source":["\n","import tensorflow\n","print(tensorflow.__version__)\n","import keras\n","print(keras.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.0\n","2.3.1\n"]},{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9OPvalush89","executionInfo":{"status":"ok","timestamp":1613060423100,"user_tz":-330,"elapsed":2496,"user":{"displayName":"AK Photography","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKpaDsaPUxbpo7CixlK-fdaegUtjkdD6FylN0c=s64","userId":"11072729652084131430"}},"outputId":"935ef44f-dc3d-4987-e62a-05399d4e9396"},"source":["import string\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# extract descriptions for images\n","def load_descriptions(doc):\n","\tmapping = dict()\n","\t# process lines\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\tif len(line) < 2:\n","\t\t\tcontinue\n","\t\t# take the first token as the image id, the rest as the description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# remove filename from image id\n","\t\timage_id = image_id.split('.')[0]\n","\t\t# convert description tokens back to string\n","\t\timage_desc = ' '.join(image_desc)\n","\t\t# create the list if needed\n","\t\tif image_id not in mapping:\n","\t\t\tmapping[image_id] = list()\n","\t\t# store description\n","\t\tmapping[image_id].append(image_desc)\n","\treturn mapping\n","\n","def clean_descriptions(descriptions):\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor i in range(len(desc_list)):\n","\t\t\tdesc = desc_list[i]\n","\t\t\t# tokenize\n","\t\t\tdesc = desc.split()\n","\t\t\t# convert to lower case\n","\t\t\tdesc = [word.lower() for word in desc]\n","\t\t\t# remove punctuation from each token\n","\t\t\tdesc = [w.translate(table) for w in desc]\n","\t\t\t# remove hanging 's' and 'a'\n","\t\t\tdesc = [word for word in desc if len(word)>1]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tdesc = [word for word in desc if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tdesc_list[i] =  ' '.join(desc)\n","\n","# convert the loaded descriptions into a vocabulary of words\n","def to_vocabulary(descriptions):\n","\t# build a list of all description strings\n","\tall_desc = set()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n","\treturn all_desc\n","\n","# save descriptions to file, one per line\n","def save_descriptions(descriptions, filename):\n","\tlines = list()\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor desc in desc_list:\n","\t\t\tlines.append(key + ' ' + desc)\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()\n","\n","# filename = 'Flickr8k_text/Flickr8k.token.txt'\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr8k.token.txt'\n","# load descriptions\n","doc = load_doc(filename)\n","# parse descriptions\n","descriptions = load_descriptions(doc)\n","print('Loaded: %d ' % len(descriptions))\n","# clean descriptions\n","clean_descriptions(descriptions)\n","# summarize vocabulary\n","vocabulary = to_vocabulary(descriptions)\n","print('Vocabulary Size: %d' % len(vocabulary))\n","# save to file\n","save_descriptions(descriptions, '/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loaded: 8092 \n","Vocabulary Size: 8763\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hz7LnLFEtORX","executionInfo":{"status":"ok","timestamp":1613060424436,"user_tz":-330,"elapsed":3066,"user":{"displayName":"AK Photography","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKpaDsaPUxbpo7CixlK-fdaegUtjkdD6FylN0c=s64","userId":"11072729652084131430"}},"outputId":"664499fc-ae0f-436f-a2af-a917340ee445"},"source":["from pickle import load\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)\n","\n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n","\n","# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features\n","\n","# load training dataset (6K)\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt', train)\n","print('Descriptions train: =%d' % len(train_descriptions))\n","# photo features\n","train_features = load_photo_features('/content/drive/MyDrive/ICG/VGG16_with_LSTM/features.pkl', train)\n","print('Photos train: =%d' % len(train_features))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset: 6000\n","Descriptions train: =6000\n","Photos train: =6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZzeB7n-NtdV6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6e1f6280-e22f-4ba9-c757-fa5c71365766"},"source":["from numpy import array\n","import tensorflow\n","from pickle import load\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils import plot_model\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.layers import Dropout\n","from keras.layers.merge import add\n","from keras.callbacks import ModelCheckpoint\n","\n","\n","\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)\n","\n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n","\n","# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features\n","\n","\n","\n","# covert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n","\n","# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = to_lines(descriptions)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","\n","\n","# calculate the length of the description with the most words\n","def max_length(descriptions):\n","\tlines = to_lines(descriptions)\n","\treturn max(len(d.split()) for d in lines)\n","\n","# create sequences of images, input sequences and output words for an image\n","def create_sequences(tokenizer, max_length, desc_list, photo):\n","\tX1, X2, y = list(), list(), list()\n","\t# walk through each description for the image\n","\tfor desc in desc_list:\n","\t\t# encode the sequence\n","\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n","\t\t# split one sequence into multiple X,y pairs\n","\t\tfor i in range(1, len(seq)):\n","\t\t\t# split into input and output pair\n","\t\t\tin_seq, out_seq = seq[:i], seq[i]\n","\t\t\t# pad input sequence\n","\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","\t\t\t# encode output sequence\n","\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\t\t\t# store\n","\t\t\tX1.append(photo)\n","\t\t\tX2.append(in_seq)\n","\t\t\ty.append(out_seq)\n","\treturn array(X1), array(X2), array(y)\n","\n","# define the captioning model\n","def define_model(vocab_size, max_length):\n","\t# feature extractor model\n","\tinputs1 = Input(shape=(1000,))\n","\tfe1 = Dropout(0.4)(inputs1)\n","\tfe2 = Dense(256, activation='relu')(fe1)\n","\t# sequence model\n","\tinputs2 = Input(shape=(max_length,))\n","\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","\tse2 = Dropout(0.4)(se1)\n","\tse3 = LSTM(256)(se2)\n","\t# decoder model\n","\tdecoder1 = add([fe2, se3])\n","\tdecoder2 = Dense(256, activation='relu')(decoder1)\n","\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\t# tie it together [image, seq] [word]\n","\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\t# summarize model\n","\tprint(model.summary())\n","\t\n","    \n","\treturn model\n","\n","#Below code is used to progressively load the batch of data\n","# data generator, intended to be used in a call to model.fit_generator()\n","def data_generator(descriptions, photos, tokenizer, max_length):\n","\t# loop for ever over images\n","\twhile 1:\n","\t\tfor key, desc_list in descriptions.items():\n","\t\t\t# retrieve the photo feature\n","\t\t\tphoto = photos[key][0]\n","\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n","\t\t\tyield [[in_img, in_seq], out_word]\n","\n","\t\t\t\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt', train)\n","print('Descriptions train: =%d' % len(train_descriptions))\n","# photo features\n","train_features = load_photo_features('/content/drive/MyDrive/ICG/VGG16_with_LSTM/features.pkl', train)\n","print('Photos train: =%d' % len(train_features))\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)\n","# determine the maximum sequence length\n","max_length = max_length(train_descriptions)\n","print('Description Length: %d' % max_length)\n","\n","# train the model\n","model = define_model(vocab_size, max_length)\n","\n","# train the model, run epochs manually and save after each epoch\n","#epochs = 10\n","steps = len(train_descriptions)\n","\n","for i in range(epochs):\n","\t# create the data generator\n","\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n","\t# fit for one epoch\n","\tmodel.fit_generator(generator, epochs=5, steps_per_epoch=steps, verbose=1)\n","\t# save model\n","\tmodel.save('/content/drive/MyDrive/ICG/VGG16_with_LSTM/0.51/model' + str(i+1) + '.h5')\n","\n","print(\"Saving model...\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset: 6000\n","Descriptions train: =6000\n","Photos train: =6000\n","Vocabulary Size: 7579\n","Description Length: 34\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 34)           0                                            \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 1000)         0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 34, 256)      1940224     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 1000)         0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          256256      dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n","                                                                 lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n","==================================================================================================\n","Total params: 4,735,387\n","Trainable params: 4,735,387\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","6000/6000 [==============================] - 2203s 367ms/step - loss: 4.5970 - accuracy: 0.2280\n","Epoch 2/10\n","6000/6000 [==============================] - 2225s 371ms/step - loss: 3.7648 - accuracy: 0.2835\n","Epoch 3/10\n","6000/6000 [==============================] - 2188s 365ms/step - loss: 3.4579 - accuracy: 0.3046\n","Epoch 4/10\n","6000/6000 [==============================] - 2134s 356ms/step - loss: 3.2577 - accuracy: 0.3185\n","Epoch 5/10\n","6000/6000 [==============================] - 2212s 369ms/step - loss: 3.1127 - accuracy: 0.3294\n","Epoch 6/10\n"," 230/6000 [>.............................] - ETA: 33:20 - loss: 3.0052 - accuracy: 0.3329"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QKrPc9XDBi1w"},"source":["import matplotlib.pyplot as plt\n","accuracy = history.History['accuracy']\n","epochs = range(len(accuracy))\n","plt.plot(epochs, accuracy, 'b', label='accuracy')\n","plt.title('Train accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amc23KRrCRYe"},"source":["import matplotlib.pyplot as plt\n","accuracy = history.History['loss']\n","epochs = range(len(loss))\n","plt.plot(epochs, loss, 'b', label='loss')\n","plt.title('Train loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AaVmIvUJ7FSN","executionInfo":{"status":"ok","timestamp":1615369442904,"user_tz":-330,"elapsed":4228,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"29368fad-3fba-4df2-a41b-0487994e8fc5"},"source":["import nltk\n","!pip install rouge\n","print(nltk.__version__)\n","!pip install nltk==3.4.5\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3.4.5\n","Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (3.4.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5) (1.15.0)\n"],"name":"stdout"},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkPlkzZItzQ9","executionInfo":{"status":"ok","timestamp":1615369662168,"user_tz":-330,"elapsed":38832,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"36760264-63a3-4d4e-abbc-829e87634e6c"},"source":["          \n","from numpy import argmax\n","from pickle import load\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n","import nltk\n","from rouge import Rouge \n","\n","\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)\n","\n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n","\n","# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features\n","\n","# covert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n","\n","# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = to_lines(descriptions)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","\n","\n","# calculate the length of the description with the most words\n","def max_length(descriptions):\n","\tlines = to_lines(descriptions)\n","\treturn max(len(d.split()) for d in lines)\n","\n","# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n","\n","# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","\t# seed the generation process\n","\tin_text = 'startseq'\n","\t# iterate over the whole length of the sequence\n","\tfor i in range(max_length):\n","\t\t# integer encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad input\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\n","\t\t# convert probability to integer\n","\t\tyhat = argmax(yhat)\n","\t\t# map integer to word\n","\t\tword = word_for_id(yhat, tokenizer)\n","\t\t# stop if we cannot map the word\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append as input for generating the next word\n","\t\tin_text += ' ' + word\n","\t\t# stop if we predict the end of the sequence\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\treturn in_text\n","\n","\n","# evaluate the skill of the model\n","def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = list(), list()\n","\t# step over the whole set\n","\tfor key, desc in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\treferences = [d.split() for d in desc]\n","\t\tactual.append(references)\n","\t\tpredicted.append(yhat.split())\n","\t\t#print('Actual:    %s' % desc)\n","\t\t#print('Predicted: %s' % yhat)\n","\t\tif len(actual) >= 5:\n","\t\t\tbreak\n","\n","\t# calculate BLEU score\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"," \n","\n","def evaluate_ROUGE(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = list(), list()\n","\t# step over the whole set\n","\tfor key, desc in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\t#references = [d.split() for d in desc]\n","\t\t#actual.join(references)\n","\t\tactual = desc\n","\t\tpredicted = yhat\n","\t\t#predicted.join(yhat)\n","\t\t\n","\t\t#print('Actual:    %s' %actual)\n","\t\t#print('Predected:  %s' %predicted)\n","\t\tif len(actual) >= 5:\n","\t\t\tbreak\n","\t\t\n","\t\t\n","\t# calculate ROUGE score\n","\trouge = Rouge()\n","\tscores = rouge.get_scores(predicted, \", \".join(actual), avg=True)\n","\tprint('ROUGE: ', scores)\n","\n","\n","def evaluate_METEOR(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = '',''\n","\t# step over the whole set\n","\t\n","\tfor key, desc in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\t#references = [d.split() for d in desc]\n","\t\t#actual.join(references)\n","\t\tactual = desc\n","\t\tpredicted = yhat\n","\t\t#predicted.join(yhat)\n","\t\t\n","\t\t#print('Actual:    %s' %actual)\n","\t\t#print('Predected:  %s' %predicted)\n","\t\tif len(actual) >= 5:\n","\t\t\tbreak\n","\t\t\n","\t\t\n","\t# calculate METEOR score\n","\tprint('METEOR: %f' % nltk.translate.meteor_score.meteor_score(actual, predicted))\n","\t\t\n","\n","# prepare training set\n","\n","# load training dataset (6K)\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Training Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt', train)\n","print('Descriptions train: =%d' % len(train_descriptions))\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)\n","# determine the maximum sequence length\n","max_length = max_length(train_descriptions)\n","print('Description Length: %d' % max_length)\n","\n","\n","# prepare test set\n","\n","# load test set\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.testImages.txt'\n","test = load_set(filename)\n","print('Testing Dataset: %d' % len(test))\n","# descriptions\n","test_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt', test)\n","print('Descriptions test: =%d' % len(test_descriptions))\n","# photo features\n","test_features = load_photo_features('/content/drive/MyDrive/ICG/VGG16_with_LSTM/features.pkl', test)\n","print('Photos test: =%d' % len(test_features))\n","\n","\n","models = 10\n","for i in range(models):\n","  # load the model which has minimum loss, in this case it was model_1\n","  print('BLEU Score of epoch ' + str(i+1) )\n","  filename = '/content/drive/MyDrive/ICG/VGG16_with_LSTM/model(0.5)' + str(i+1) + '.h5'\n","  model = load_model(filename)\n","  # evaluate model\n","  evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\n","  evaluate_METEOR(model, test_descriptions, test_features, tokenizer, max_length)\n","  evaluate_ROUGE(model, test_descriptions, test_features, tokenizer, max_length)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Dataset: 6000\n","Descriptions train: =6000\n","Vocabulary Size: 7579\n","Description Length: 34\n","Testing Dataset: 1000\n","Descriptions test: =1000\n","Photos test: =1000\n","BLEU Score of epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.576923\n","BLEU-2: 0.332377\n","BLEU-3: 0.255048\n","BLEU-4: 0.129858\n","METEOR: 0.506051\n","ROUGE:  {'rouge-1': {'f': 0.2025316431725685, 'p': 0.7272727272727273, 'r': 0.11764705882352941}, 'rouge-2': {'f': 0.05194804968797446, 'p': 0.2, 'r': 0.029850746268656716}, 'rouge-l': {'f': 0.3076923043565089, 'p': 0.7272727272727273, 'r': 0.1951219512195122}}\n","BLEU Score of epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.574468\n","BLEU-2: 0.330791\n","BLEU-3: 0.264174\n","BLEU-4: 0.000000\n","METEOR: 0.506051\n","ROUGE:  {'rouge-1': {'f': 0.2025316431725685, 'p': 0.7272727272727273, 'r': 0.11764705882352941}, 'rouge-2': {'f': 0.05194804968797446, 'p': 0.2, 'r': 0.029850746268656716}, 'rouge-l': {'f': 0.3076923043565089, 'p': 0.7272727272727273, 'r': 0.1951219512195122}}\n","BLEU Score of epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.535714\n","BLEU-2: 0.307470\n","BLEU-3: 0.192382\n","BLEU-4: 0.000000\n","METEOR: 0.206612\n","ROUGE:  {'rouge-1': {'f': 0.14814814545343702, 'p': 0.46153846153846156, 'r': 0.08823529411764706}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.22641509083659667, 'p': 0.5, 'r': 0.14634146341463414}}\n","BLEU Score of epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.451613\n","BLEU-2: 0.251762\n","BLEU-3: 0.164476\n","BLEU-4: 0.000000\n","METEOR: 0.347664\n","ROUGE:  {'rouge-1': {'f': 0.16470587915294121, 'p': 0.4117647058823529, 'r': 0.10294117647058823}, 'rouge-2': {'f': 0.02409638242996121, 'p': 0.0625, 'r': 0.014925373134328358}, 'rouge-l': {'f': 0.2641509398932005, 'p': 0.5833333333333334, 'r': 0.17073170731707318}}\n","BLEU Score of epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.517857\n","BLEU-2: 0.302302\n","BLEU-3: 0.215067\n","BLEU-4: 0.109803\n","METEOR: 0.361165\n","ROUGE:  {'rouge-1': {'f': 0.17283950347812838, 'p': 0.5384615384615384, 'r': 0.10294117647058823}, 'rouge-2': {'f': 0.02531645311969262, 'p': 0.08333333333333333, 'r': 0.014925373134328358}, 'rouge-l': {'f': 0.25925925560356655, 'p': 0.5384615384615384, 'r': 0.17073170731707318}}\n","BLEU Score of epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.482143\n","BLEU-2: 0.257248\n","BLEU-3: 0.212815\n","BLEU-4: 0.129438\n","METEOR: 0.208333\n","ROUGE:  {'rouge-1': {'f': 0.14999999745000003, 'p': 0.5, 'r': 0.08823529411764706}, 'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0}, 'rouge-l': {'f': 0.22641509083659667, 'p': 0.5, 'r': 0.14634146341463414}}\n","BLEU Score of epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.576923\n","BLEU-2: 0.383796\n","BLEU-3: 0.314001\n","BLEU-4: 0.183647\n","METEOR: 0.361165\n","ROUGE:  {'rouge-1': {'f': 0.17283950347812838, 'p': 0.5384615384615384, 'r': 0.10294117647058823}, 'rouge-2': {'f': 0.02531645311969262, 'p': 0.08333333333333333, 'r': 0.014925373134328358}, 'rouge-l': {'f': 0.25925925560356655, 'p': 0.5384615384615384, 'r': 0.17073170731707318}}\n","BLEU Score of epoch 8\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.551020\n","BLEU-2: 0.274115\n","BLEU-3: 0.188690\n","BLEU-4: 0.000000\n","METEOR: 0.361165\n","ROUGE:  {'rouge-1': {'f': 0.17283950347812838, 'p': 0.5384615384615384, 'r': 0.10294117647058823}, 'rouge-2': {'f': 0.02531645311969262, 'p': 0.08333333333333333, 'r': 0.014925373134328358}, 'rouge-l': {'f': 0.25925925560356655, 'p': 0.5384615384615384, 'r': 0.17073170731707318}}\n","BLEU Score of epoch 9\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.418182\n","BLEU-2: 0.204495\n","BLEU-3: 0.151619\n","BLEU-4: 0.082562\n","METEOR: 0.346658\n","ROUGE:  {'rouge-1': {'f': 0.16867469583393818, 'p': 0.4666666666666667, 'r': 0.10294117647058823}, 'rouge-2': {'f': 0.024691355165371465, 'p': 0.07142857142857142, 'r': 0.014925373134328358}, 'rouge-l': {'f': 0.22641509083659667, 'p': 0.5, 'r': 0.14634146341463414}}\n","BLEU Score of epoch 10\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.580000\n","BLEU-2: 0.321109\n","BLEU-3: 0.205912\n","BLEU-4: 0.000000\n","METEOR: 0.413970\n","ROUGE:  {'rouge-1': {'f': 0.1975308615028197, 'p': 0.6153846153846154, 'r': 0.11764705882352941}, 'rouge-2': {'f': 0.0759493645120975, 'p': 0.25, 'r': 0.04477611940298507}, 'rouge-l': {'f': 0.29629629264060364, 'p': 0.6153846153846154, 'r': 0.1951219512195122}}\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HqYvLNZB0squ","executionInfo":{"status":"ok","timestamp":1615894612475,"user_tz":-330,"elapsed":13395,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"ecc78a2a-916c-4c02-ac99-77ec76062d3f"},"source":["!pip install rouge\n","!pip install nltk==3.4.5\n","import nltk\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting rouge\n","  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.0\n","Collecting nltk==3.4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5) (1.15.0)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449908 sha256=895a325f266091e6301f24ad6f3d6a823f756576777283079537c0356d0f1bfe\n","  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n","Successfully built nltk\n","Installing collected packages: nltk\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.4.5\n"],"name":"stdout"},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"J3nYS4xB0uQB"},"source":["\n","from numpy import argmax\n","from pickle import load\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n","import nltk\n","from rouge import Rouge \n","\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)\n","\n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n","\n","# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features\n","\n","# covert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n","\n","# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = to_lines(descriptions)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","\n","\n","# calculate the length of the description with the most words\n","def max_length(descriptions):\n","\tlines = to_lines(descriptions)\n","\treturn max(len(d.split()) for d in lines)\n","\n","# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n","\n","\n","\n","# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","\t# seed the generation process\n","\tin_text = 'startseq'\n","\t# iterate over the whole length of the sequence\n","\tfor i in range(max_length):\n","\t\t# integer encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad input\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\n","\t\t# convert probability to integer\n","\t\tyhat = argmax(yhat)\n","\t\t# map integer to word\n","\t\tword = word_for_id(yhat, tokenizer)\n","\t\t# stop if we cannot map the word\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append as input for generating the next word\n","\t\tin_text += ' ' + word\n","\t\t# stop if we predict the end of the sequence\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\treturn in_text\n","\n","\n","\n","\n","\n","def evaluate_ROUGE(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = list(), list()\n","\t# step over the whole set\n","\tfor key, desc in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\t#references = [d.split() for d in desc]\n","\t\t#actual.join(references)\n","\t\tactual = desc\n","\t\tpredicted = yhat\n","\t\t#predicted.join(yhat)\n","\t\t\n","\t\t#print('Actual:    %s' %actual)\n","\t\t#print('Predected:  %s' %predicted)\n","\t\tif len(actual) >= 5:\n","\t\t\tbreak\n","\t\t\n","\t\t\n","\t# calculate ROUGE score\n","\trouge = Rouge()\n","\tscores = rouge.get_scores(predicted, \", \".join(actual), avg=True)\n","\tprint('ROUGE: ', scores)\n","\n","\n","def evaluate_METEOR(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = '',''\n","\t# step over the whole set\n","\t\n","\tfor key, desc in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\t#references = [d.split() for d in desc]\n","\t\t#actual.join(references)\n","\t\tactual = desc\n","\t\tpredicted = yhat\n","\t\t#predicted.join(yhat)\n","\t\t\n","\t\t#print('Actual:    %s' %actual)\n","\t\t#print('Predected:  %s' %predicted)\n","\t\tif len(actual) >= 5:\n","\t\t\tbreak\n","\t\t\n","\t\t\n","\t# calculate METEOR score\n","\tprint('METEOR: %f' % nltk.translate.meteor_score.meteor_score(actual, predicted))\n","\t\t\n","\n","# prepare training set\n","\n","# load training dataset (6K)\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Training Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt', train)\n","print('Descriptions train: =%d' % len(train_descriptions))\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)\n","# determine the maximum sequence length\n","max_length = max_length(train_descriptions)\n","print('Description Length: %d' % max_length)\n","\n","\n","# prepare test set\n","\n","# load test set\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.testImages.txt'\n","test = load_set(filename)\n","print('Testing Dataset: %d' % len(test))\n","# descriptions\n","test_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt', test)\n","print('Descriptions test: =%d' % len(test_descriptions))\n","# photo features\n","test_features = load_photo_features('/content/drive/MyDrive/ICG/VGG16_with_LSTM/features.pkl', test)\n","print('Photos test: =%d' % len(test_features))\n","\n","# load the model which has minimum loss, in this case it was model_1\n","models = 10\n","for i in range(models):\n","  # load the model which has minimum loss, in this case it was model_1\n","  filename = '/content/drive/MyDrive/ICG/VGG16_with_LSTM/0.5/model' + str(i+1) + '.h5'\n","  model = load_model(filename)\n","  # evaluate model\n","  print('METEOR Score of epoch ' + str(i+1) )\n","  evaluate_METEOR(model, test_descriptions, test_features, tokenizer, max_length)\n","  print('ROUGE Score of epoch ' + str(i+1) )\n","  evaluate_ROUGE(model, test_descriptions, test_features, tokenizer, max_length)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GImfR664t_TG","executionInfo":{"status":"ok","timestamp":1614135183659,"user_tz":-330,"elapsed":1585,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"fb188114-e246-442b-8398-06eb80303667"},"source":["from keras.preprocessing.text import Tokenizer\n","from pickle import dump\n","\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text \n","\n","# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)\n","\n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n","\n","\n","# covert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n"," \n","# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = to_lines(descriptions)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n"," \n","# load training dataset (6K)\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt', train)\n","print('Descriptions train: =%d' % len(train_descriptions))\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","# save the tokenizer\n","dump(tokenizer, open('/content/drive/MyDrive/ICG/VGG16_with_LSTM/tokenizer.pkl', 'wb'))\n","print('Tokenizer.pkl file save....')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset: 6000\n","Descriptions train: =6000\n","Tokenizer.pkl file save....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJyxIjSYuOaa","executionInfo":{"status":"ok","timestamp":1614135364091,"user_tz":-330,"elapsed":5849,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"820e2eda-b140-4fe7-f1c0-5444dfa30c34"},"source":["#Generate Captions for a Fresh Image\n","\n","from pickle import load\n","from numpy import argmax\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.applications.vgg16 import VGG16\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from keras.models import Model\n","from keras.models import load_model\n","\n","# extract features from each photo in the directory\n","def extract_features(filename):\n","\t# load the model\n","\tmodel = VGG16()\n","\t# re-structure the model\n","\tmodel.layers.pop()\n","\t#model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n","\t# load the photo\n","\timage = load_img(filename, target_size=(224, 224))\n","\t# convert the image pixels to a numpy array\n","\timage = img_to_array(image)\n","\t# reshape data for the model\n","\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","\t# prepare the image for the VGG model\n","\timage = preprocess_input(image)\n","\t# get features\n","\tfeature = model.predict(image, verbose=0)\n","\treturn feature\n","\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n","\n","# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","\t# seed the generation process\n","\tin_text = 'startseq'\n","\t# iterate over the whole length of the sequence\n","\tfor i in range(max_length):\n","\t\t# integer encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad input\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\n","\t\t# convert probability to integer\n","\t\tyhat = argmax(yhat)\n","\t\t# map integer to word\n","\t\tword = word_for_id(yhat, tokenizer)\n","\t\t# stop if we cannot map the word\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append as input for generating the next word\n","\t\tin_text += ' ' + word\n","\t\t# stop if we predict the end of the sequence\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\treturn in_text\n","\n","# load the tokenizer\n","tokenizer = load(open('/content/drive/MyDrive/ICG/VGG16_with_LSTM/tokenizer.pkl', 'rb'))\n","# pre-define the max sequence length (from training)\n","max_length = 34\n","# load the model\n","model = load_model('/content/drive/MyDrive/ICG/VGG16_with_LSTM/model(0.5)1.h5')\n","# load and prepare the photograph\n","photo = extract_features('/content/drive/MyDrive/ICG/example.jpg')\n","# generate description\n","description = generate_desc(model, tokenizer, photo, max_length)\n","print(\"Caption of example image: \")\n","print(description)\n","\n","#Remove startseq and endseq\n","query = description\n","stopwords = ['startseq','endseq']\n","querywords = query.split()\n","\n","resultwords  = [word for word in querywords if word.lower() not in stopwords]\n","result = ' '.join(resultwords)\n","\n","print(\"Clean Caption of image: \")\n","print(result)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Caption of example image: \n","startseq dog is running through the water endseq\n","Clean Caption of image: \n","dog is running through the water\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MACBxtr5EyCK"},"source":[""],"execution_count":null,"outputs":[]}]}