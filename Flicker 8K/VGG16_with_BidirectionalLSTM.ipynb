{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG16_with_BidirectionalLSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ALPmiipBlm_","executionInfo":{"status":"ok","timestamp":1615893493298,"user_tz":-330,"elapsed":25474,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"e82a7884-bf04-4d9c-a751-bc1415b1e674"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2QZlVYsKT2Pn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615893547423,"user_tz":-330,"elapsed":59427,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"e3b9af9e-2372-474c-904c-56db9cb4790b"},"source":["!pip install tensorflow==2.0.0\r\n","!pip install keras==2.3.1"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/5c/f1d66de5dde6f3ff528f6ea1fd0757a0e594d17debb3ec7f82daa967ea9a/tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3MB)\n","\u001b[K     |████████████████████████████████| 86.3MB 57kB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.12.4)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.8.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.32.0)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 30.2MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.12.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.36.2)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.10.0)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 34.5MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.15.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.19.5)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (54.0.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.10.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.27.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.3.4)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.7.2)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.7.4.3)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=4f7c3c9b2ab6e6db95bc5fdbb90182f5d2cfc6062565b5aaaad2d44ed44d21b6\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: keras-applications, tensorflow-estimator, gast, tensorboard, tensorflow\n","  Found existing installation: tensorflow-estimator 2.4.0\n","    Uninstalling tensorflow-estimator-2.4.0:\n","      Successfully uninstalled tensorflow-estimator-2.4.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","  Found existing installation: tensorflow 2.4.1\n","    Uninstalling tensorflow-2.4.1:\n","      Successfully uninstalled tensorflow-2.4.1\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n","Collecting keras==2.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n","\u001b[K     |████████████████████████████████| 378kB 5.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MzmV11TssCZz"},"source":["#Import Necessary Libraries\r\n","from os import listdir\r\n","from pickle import dump\r\n","from keras.applications.vgg16 import VGG16\r\n","from keras.preprocessing.image import load_img\r\n","from keras.preprocessing.image import img_to_array\r\n","from keras.applications.vgg16 import preprocess_input\r\n","from keras.models import Model\r\n","\r\n","# extract features from each photo in the directory\r\n","def extract_features(directory):\r\n","\t# load the model\r\n","\tmodel = VGG16(include_top=True, weights='imagenet')\r\n","\t# re-structure the model\r\n","\tmodel.layers.pop()\r\n","\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\r\n","\t# summarize\r\n","\tprint(model.summary())\r\n","\t# extract features from each photo\r\n","\tfeatures = dict()\r\n","\tfor name in listdir(directory):\r\n","\t\t# load an image from file\r\n","\t\tfilename = directory + '/' + name\r\n","\t\timage = load_img(filename, target_size=(224, 224))\r\n","\t\t# convert the image pixels to a numpy array\r\n","\t\timage = img_to_array(image)\r\n","\t\t# reshape data for the model\r\n","\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\r\n","\t\t# prepare the image for the VGG model\r\n","\t\timage = preprocess_input(image)\r\n","\t\t# get features\r\n","\t\tfeature = model.predict(image, verbose=0)\r\n","\t\t# get image id\r\n","\t\timage_id = name.split('.')[0]\r\n","\t\t# store feature\r\n","\t\tfeatures[image_id] = feature\r\n","\t\tprint(len(features) ,':', name)\r\n","\treturn features\r\n","\r\n","# extract features from all images\r\n","directory = '/content/drive/MyDrive/ICG/Flickr8k/Flicker8k_Dataset'\r\n","features = extract_features(directory)\r\n","print('Extracted Features: %d' % len(features))\r\n","# save to file\r\n","dump(features, open('/content/drive/MyDrive/ICG/VGG16_with_LSTM/features_true.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBA9SMdC2-KL","executionInfo":{"status":"ok","timestamp":1615893553785,"user_tz":-330,"elapsed":2581,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"d54674d6-3a4d-4527-b9b1-c73434b9c4f0"},"source":["import tensorflow\r\n","print(tensorflow.__version__)\r\n","import keras\r\n","print(keras.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["2.0.0\n","2.3.1\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9OPvalush89","outputId":"7a9a9ec7-54c6-474d-f750-42b1a432bd52"},"source":["import string\r\n","\r\n","# load doc into memory\r\n","def load_doc(filename):\r\n","\t# open the file as read only\r\n","\tfile = open(filename, 'r')\r\n","\t# read all text\r\n","\ttext = file.read()\r\n","\t# close the file\r\n","\tfile.close()\r\n","\treturn text\r\n","\r\n","# extract descriptions for images\r\n","def load_descriptions(doc):\r\n","\tmapping = dict()\r\n","\t# process lines\r\n","\tfor line in doc.split('\\n'):\r\n","\t\t# split line by white space\r\n","\t\ttokens = line.split()\r\n","\t\tif len(line) < 2:\r\n","\t\t\tcontinue\r\n","\t\t# take the first token as the image id, the rest as the description\r\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\r\n","\t\t# remove filename from image id\r\n","\t\timage_id = image_id.split('.')[0]\r\n","\t\t# convert description tokens back to string\r\n","\t\timage_desc = ' '.join(image_desc)\r\n","\t\t# create the list if needed\r\n","\t\tif image_id not in mapping:\r\n","\t\t\tmapping[image_id] = list()\r\n","\t\t# store description\r\n","\t\tmapping[image_id].append(image_desc)\r\n","\treturn mapping\r\n","\r\n","def clean_descriptions(descriptions):\r\n","\t# prepare translation table for removing punctuation\r\n","\ttable = str.maketrans('', '', string.punctuation)\r\n","\tfor key, desc_list in descriptions.items():\r\n","\t\tfor i in range(len(desc_list)):\r\n","\t\t\tdesc = desc_list[i]\r\n","\t\t\t# tokenize\r\n","\t\t\tdesc = desc.split()\r\n","\t\t\t# convert to lower case\r\n","\t\t\tdesc = [word.lower() for word in desc]\r\n","\t\t\t# remove punctuation from each token\r\n","\t\t\tdesc = [w.translate(table) for w in desc]\r\n","\t\t\t# remove hanging 's' and 'a'\r\n","\t\t\tdesc = [word for word in desc if len(word)>1]\r\n","\t\t\t# remove tokens with numbers in them\r\n","\t\t\tdesc = [word for word in desc if word.isalpha()]\r\n","\t\t\t# store as string\r\n","\t\t\tdesc_list[i] =  ' '.join(desc)\r\n","\r\n","# convert the loaded descriptions into a vocabulary of words\r\n","def to_vocabulary(descriptions):\r\n","\t# build a list of all description strings\r\n","\tall_desc = set()\r\n","\tfor key in descriptions.keys():\r\n","\t\t[all_desc.update(d.split()) for d in descriptions[key]]\r\n","\treturn all_desc\r\n","\r\n","# save descriptions to file, one per line\r\n","def save_descriptions(descriptions, filename):\r\n","\tlines = list()\r\n","\tfor key, desc_list in descriptions.items():\r\n","\t\tfor desc in desc_list:\r\n","\t\t\tlines.append(key + ' ' + desc)\r\n","\tdata = '\\n'.join(lines)\r\n","\tfile = open(filename, 'w')\r\n","\tfile.write(data)\r\n","\tfile.close()\r\n","\r\n","# filename = 'Flickr8k_text/Flickr8k.token.txt'\r\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr8k.token.txt'\r\n","# load descriptions\r\n","doc = load_doc(filename)\r\n","# parse descriptions\r\n","descriptions = load_descriptions(doc)\r\n","print('Loaded: %d ' % len(descriptions))\r\n","# clean descriptions\r\n","clean_descriptions(descriptions)\r\n","# summarize vocabulary\r\n","vocabulary = to_vocabulary(descriptions)\r\n","print('Vocabulary Size: %d' % len(vocabulary))\r\n","# save to file\r\n","save_descriptions(descriptions, '/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt')\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loaded: 8092 \n","Vocabulary Size: 8763\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hz7LnLFEtORX","outputId":"e5efbe08-abc2-4556-ab64-cdee151ec8da"},"source":["from pickle import load\r\n","\r\n","# load doc into memory\r\n","def load_doc(filename):\r\n","\t# open the file as read only\r\n","\tfile = open(filename, 'r')\r\n","\t# read all text\r\n","\ttext = file.read()\r\n","\t# close the file\r\n","\tfile.close()\r\n","\treturn text\r\n","\r\n","# load a pre-defined list of photo identifiers\r\n","def load_set(filename):\r\n","\tdoc = load_doc(filename)\r\n","\tdataset = list()\r\n","\t# process line by line\r\n","\tfor line in doc.split('\\n'):\r\n","\t\t# skip empty lines\r\n","\t\tif len(line) < 1:\r\n","\t\t\tcontinue\r\n","\t\t# get the image identifier\r\n","\t\tidentifier = line.split('.')[0]\r\n","\t\tdataset.append(identifier)\r\n","\treturn set(dataset)\r\n","\r\n","# load clean descriptions into memory\r\n","def load_clean_descriptions(filename, dataset):\r\n","\t# load document\r\n","\tdoc = load_doc(filename)\r\n","\tdescriptions = dict()\r\n","\tfor line in doc.split('\\n'):\r\n","\t\t# split line by white space\r\n","\t\ttokens = line.split()\r\n","\t\t# split id from description\r\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\r\n","\t\t# skip images not in the set\r\n","\t\tif image_id in dataset:\r\n","\t\t\t# create list\r\n","\t\t\tif image_id not in descriptions:\r\n","\t\t\t\tdescriptions[image_id] = list()\r\n","\t\t\t# wrap description in tokens\r\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\r\n","\t\t\t# store\r\n","\t\t\tdescriptions[image_id].append(desc)\r\n","\treturn descriptions\r\n","\r\n","# load photo features\r\n","def load_photo_features(filename, dataset):\r\n","\t# load all features\r\n","\tall_features = load(open(filename, 'rb'))\r\n","\t# filter features\r\n","\tfeatures = {k: all_features[k] for k in dataset}\r\n","\treturn features\r\n","\r\n","# load training dataset (6K)\r\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.trainImages.txt'\r\n","train = load_set(filename)\r\n","print('Dataset: %d' % len(train))\r\n","# descriptions\r\n","train_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_LSTM/descriptions.txt', train)\r\n","print('Descriptions train: =%d' % len(train_descriptions))\r\n","# photo features\r\n","train_features = load_photo_features('/content/drive/MyDrive/ICG/VGG16_with_LSTM/features_true.pkl', train)\r\n","print('Photos train: =%d' % len(train_features))\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset: 6000\n","Descriptions train: =6000\n","Photos train: =6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZzeB7n-NtdV6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"993088a9-ba03-440b-fbf0-0945c997542b"},"source":["from numpy import array\r\n","import tensorflow\r\n","from pickle import load\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.utils import to_categorical\r\n","from keras.utils import plot_model\r\n","from keras.models import Model\r\n","from keras.layers import Input\r\n","from keras.layers import Dense\r\n","from keras.layers import LSTM\r\n","from keras.layers import Bidirectional\r\n","from keras.layers import Embedding\r\n","from keras.layers import Dropout\r\n","from keras.layers.merge import add\r\n","from keras.callbacks import ModelCheckpoint\r\n","\r\n","\r\n","\r\n","\r\n","# load doc into memory\r\n","def load_doc(filename):\r\n","\t# open the file as read only\r\n","\tfile = open(filename, 'r')\r\n","\t# read all text\r\n","\ttext = file.read()\r\n","\t# close the file\r\n","\tfile.close()\r\n","\treturn text\r\n","\r\n","# load a pre-defined list of photo identifiers\r\n","def load_set(filename):\r\n","\tdoc = load_doc(filename)\r\n","\tdataset = list()\r\n","\t# process line by line\r\n","\tfor line in doc.split('\\n'):\r\n","\t\t# skip empty lines\r\n","\t\tif len(line) < 1:\r\n","\t\t\tcontinue\r\n","\t\t# get the image identifier\r\n","\t\tidentifier = line.split('.')[0]\r\n","\t\tdataset.append(identifier)\r\n","\treturn set(dataset)\r\n","\r\n","# load clean descriptions into memory\r\n","def load_clean_descriptions(filename, dataset):\r\n","\t# load document\r\n","\tdoc = load_doc(filename)\r\n","\tdescriptions = dict()\r\n","\tfor line in doc.split('\\n'):\r\n","\t\t# split line by white space\r\n","\t\ttokens = line.split()\r\n","\t\t# split id from description\r\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\r\n","\t\t# skip images not in the set\r\n","\t\tif image_id in dataset:\r\n","\t\t\t# create list\r\n","\t\t\tif image_id not in descriptions:\r\n","\t\t\t\tdescriptions[image_id] = list()\r\n","\t\t\t# wrap description in tokens\r\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\r\n","\t\t\t# store\r\n","\t\t\tdescriptions[image_id].append(desc)\r\n","\treturn descriptions\r\n","\r\n","# load photo features\r\n","def load_photo_features(filename, dataset):\r\n","\t# load all features\r\n","\tall_features = load(open(filename, 'rb'))\r\n","\t# filter features\r\n","\tfeatures = {k: all_features[k] for k in dataset}\r\n","\treturn features\r\n","\r\n","\r\n","\r\n","# covert a dictionary of clean descriptions to a list of descriptions\r\n","def to_lines(descriptions):\r\n","\tall_desc = list()\r\n","\tfor key in descriptions.keys():\r\n","\t\t[all_desc.append(d) for d in descriptions[key]]\r\n","\treturn all_desc\r\n","\r\n","# fit a tokenizer given caption descriptions\r\n","def create_tokenizer(descriptions):\r\n","\tlines = to_lines(descriptions)\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n","\r\n","\r\n","\r\n","# calculate the length of the description with the most words\r\n","def max_length(descriptions):\r\n","\tlines = to_lines(descriptions)\r\n","\treturn max(len(d.split()) for d in lines)\r\n","\r\n","# create sequences of images, input sequences and output words for an image\r\n","def create_sequences(tokenizer, max_length, desc_list, photo):\r\n","\tX1, X2, y = list(), list(), list()\r\n","\t# walk through each description for the image\r\n","\tfor desc in desc_list:\r\n","\t\t# encode the sequence\r\n","\t\tseq = tokenizer.texts_to_sequences([desc])[0]\r\n","\t\t# split one sequence into multiple X,y pairs\r\n","\t\tfor i in range(1, len(seq)):\r\n","\t\t\t# split into input and output pair\r\n","\t\t\tin_seq, out_seq = seq[:i], seq[i]\r\n","\t\t\t# pad input sequence\r\n","\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\r\n","\t\t\t# encode output sequence\r\n","\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\r\n","\t\t\t# store\r\n","\t\t\tX1.append(photo)\r\n","\t\t\tX2.append(in_seq)\r\n","\t\t\ty.append(out_seq)\r\n","\treturn array(X1), array(X2), array(y)\r\n","\r\n","# define the captioning model\r\n","def define_model(vocab_size, max_length):\r\n","\t# feature extractor model\r\n","\tinputs1 = Input(shape=(1000,))\r\n","\tfe1 = Dropout(0.5)(inputs1)\r\n","\tfe2 = Dense(512, activation='relu')(fe1)\r\n","\t# sequence model\r\n","\tinputs2 = Input(shape=(max_length,))\r\n","\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\r\n","\tse2 = Dropout(0.5)(se1)\r\n","\tse3 = Bidirectional(LSTM(256))(se2)\r\n","\t# decoder model\r\n","\tdecoder1 = add([fe2, se3])\r\n","\tdecoder2 = Dense(256, activation='relu')(decoder1)\r\n","\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\r\n","\t# tie it together [image, seq] [word]\r\n","\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\r\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","\t# summarize model\r\n","\tprint(model.summary())\r\n","\t\r\n","    \r\n","\treturn model\r\n","\r\n","#Below code is used to progressively load the batch of data\r\n","# data generator, intended to be used in a call to model.fit_generator()\r\n","def data_generator(descriptions, photos, tokenizer, max_length):\r\n","\t# loop for ever over images\r\n","\twhile 1:\r\n","\t\tfor key, desc_list in descriptions.items():\r\n","\t\t\t# retrieve the photo feature\r\n","\t\t\tphoto = photos[key][0]\r\n","\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\r\n","\t\t\tyield [[in_img, in_seq], out_word]\r\n","\r\n","\t\t\t\r\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.trainImages.txt'\r\n","train = load_set(filename)\r\n","print('Dataset: %d' % len(train))\r\n","# descriptions\r\n","train_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/descriptions.txt', train)\r\n","print('Descriptions train: =%d' % len(train_descriptions))\r\n","# photo features\r\n","train_features = load_photo_features('/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/features.pkl', train)\r\n","print('Photos train: =%d' % len(train_features))\r\n","# prepare tokenizer\r\n","tokenizer = create_tokenizer(train_descriptions)\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print('Vocabulary Size: %d' % vocab_size)\r\n","# determine the maximum sequence length\r\n","max_length = max_length(train_descriptions)\r\n","print('Description Length: %d' % max_length)\r\n","\r\n","# train the model\r\n","model = define_model(vocab_size, max_length)\r\n","\r\n","# train the model, run epochs manually and save after each epoch\r\n","epochs = 10\r\n","steps = len(train_descriptions)\r\n","for i in range(epochs):\r\n","\t# create the data generator\r\n","\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length)\r\n","\t# fit for one epoch\r\n","\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\r\n","\t# save model\r\n","\tmodel.save('/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/0.5/model' + str(i+1) + '.h5')\r\n","\r\n","print(\"Model Saved...\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Dataset: 6000\n","Descriptions train: =6000\n","Photos train: =6000\n","Vocabulary Size: 7579\n","Description Length: 34\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 34)           0                                            \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 1000)         0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 34, 256)      1940224     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 1000)         0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 512)          512512      dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 512)          1050624     dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 512)          0           dense_1[0][0]                    \n","                                                                 bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 256)          131328      add_1[0][0]                      \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n","==================================================================================================\n","Total params: 5,582,491\n","Trainable params: 5,582,491\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1\n","6000/6000 [==============================] - 3730s 622ms/step - loss: 4.6198 - accuracy: 0.2253\n","Epoch 1/1\n","6000/6000 [==============================] - 3748s 625ms/step - loss: 3.8058 - accuracy: 0.2803\n","Epoch 1/1\n","6000/6000 [==============================] - 3672s 612ms/step - loss: 3.5148 - accuracy: 0.3001\n","Epoch 1/1\n","6000/6000 [==============================] - 3696s 616ms/step - loss: 3.3270 - accuracy: 0.3127\n","Epoch 1/1\n","6000/6000 [==============================] - 3949s 658ms/step - loss: 3.1912 - accuracy: 0.3229\n","Epoch 1/1\n","6000/6000 [==============================] - 4714s 786ms/step - loss: 3.0814 - accuracy: 0.3308\n","Epoch 1/1\n","5640/6000 [===========================>..] - ETA: 4:10 - loss: 2.9972 - accuracy: 0.3380"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BkPlkzZItzQ9","executionInfo":{"status":"error","timestamp":1615772634200,"user_tz":-330,"elapsed":38452,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"fb4bd40d-970a-4c1d-abf1-b5ead8b80169"},"source":["          \r\n","from numpy import argmax\r\n","from pickle import load\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.models import load_model\r\n","from nltk.translate.bleu_score import corpus_bleu\r\n","\r\n","\r\n","\r\n","# load doc into memory\r\n","def load_doc(filename):\r\n","\t# open the file as read only\r\n","\tfile = open(filename, 'r')\r\n","\t# read all text\r\n","\ttext = file.read()\r\n","\t# close the file\r\n","\tfile.close()\r\n","\treturn text\r\n","\r\n","# load a pre-defined list of photo identifiers\r\n","def load_set(filename):\r\n","\tdoc = load_doc(filename)\r\n","\tdataset = list()\r\n","\t# process line by line\r\n","\tfor line in doc.split('\\n'):\r\n","\t\t# skip empty lines\r\n","\t\tif len(line) < 1:\r\n","\t\t\tcontinue\r\n","\t\t# get the image identifier\r\n","\t\tidentifier = line.split('.')[0]\r\n","\t\tdataset.append(identifier)\r\n","\treturn set(dataset)\r\n","\r\n","# load clean descriptions into memory\r\n","def load_clean_descriptions(filename, dataset):\r\n","\t# load document\r\n","\tdoc = load_doc(filename)\r\n","\tdescriptions = dict()\r\n","\tfor line in doc.split('\\n'):\r\n","\t\t# split line by white space\r\n","\t\ttokens = line.split()\r\n","\t\t# split id from description\r\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\r\n","\t\t# skip images not in the set\r\n","\t\tif image_id in dataset:\r\n","\t\t\t# create list\r\n","\t\t\tif image_id not in descriptions:\r\n","\t\t\t\tdescriptions[image_id] = list()\r\n","\t\t\t# wrap description in tokens\r\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\r\n","\t\t\t# store\r\n","\t\t\tdescriptions[image_id].append(desc)\r\n","\treturn descriptions\r\n","\r\n","# load photo features\r\n","def load_photo_features(filename, dataset):\r\n","\t# load all features\r\n","\tall_features = load(open(filename, 'rb'))\r\n","\t# filter features\r\n","\tfeatures = {k: all_features[k] for k in dataset}\r\n","\treturn features\r\n","\r\n","# covert a dictionary of clean descriptions to a list of descriptions\r\n","def to_lines(descriptions):\r\n","\tall_desc = list()\r\n","\tfor key in descriptions.keys():\r\n","\t\t[all_desc.append(d) for d in descriptions[key]]\r\n","\treturn all_desc\r\n","\r\n","# fit a tokenizer given caption descriptions\r\n","def create_tokenizer(descriptions):\r\n","\tlines = to_lines(descriptions)\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n","\r\n","\r\n","\r\n","# calculate the length of the description with the most words\r\n","def max_length(descriptions):\r\n","\tlines = to_lines(descriptions)\r\n","\treturn max(len(d.split()) for d in lines)\r\n","\r\n","# map an integer to a word\r\n","def word_for_id(integer, tokenizer):\r\n","\tfor word, index in tokenizer.word_index.items():\r\n","\t\tif index == integer:\r\n","\t\t\treturn word\r\n","\treturn None\r\n","\r\n","# generate a description for an image\r\n","def generate_desc(model, tokenizer, photo, max_length):\r\n","\t# seed the generation process\r\n","\tin_text = 'startseq'\r\n","\t# iterate over the whole length of the sequence\r\n","\tfor i in range(max_length):\r\n","\t\t# integer encode input sequence\r\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\r\n","\t\t# pad input\r\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\r\n","\t\t# predict next word\r\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\r\n","\t\t# convert probability to integer\r\n","\t\tyhat = argmax(yhat)\r\n","\t\t# map integer to word\r\n","\t\tword = word_for_id(yhat, tokenizer)\r\n","\t\t# stop if we cannot map the word\r\n","\t\tif word is None:\r\n","\t\t\tbreak\r\n","\t\t# append as input for generating the next word\r\n","\t\tin_text += ' ' + word\r\n","\t\t# stop if we predict the end of the sequence\r\n","\t\tif word == 'endseq':\r\n","\t\t\tbreak\r\n","\treturn in_text\r\n","\r\n","# evaluate the skill of the model\r\n","def evaluate_model(model, descriptions, photos, tokenizer, max_length):\r\n","\tactual, predicted = list(), list()\r\n","\t# step over the whole set\r\n","\tfor key, desc in descriptions.items():\r\n","\t\t# generate description\r\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\r\n","\t\t# store actual and predicted\r\n","\t\treferences = [d.split() for d in desc]\r\n","\t\tactual.append(references)\r\n","\t\tpredicted.append(yhat.split())\r\n","\t\t#print('Actual:    %s' % desc)\r\n","\t\t#print('Predicted: %s' % yhat)\r\n","\t\tif len(actual) >= 5:\r\n","\t\t\tbreak\r\n","\t# calculate BLEU score\r\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\r\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\r\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0)))\r\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\r\n","\r\n","\r\n","# prepare training set\r\n","\r\n","# load training dataset (6K)\r\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.trainImages.txt'\r\n","train = load_set(filename)\r\n","print('Training Dataset: %d' % len(train))\r\n","# descriptions\r\n","train_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/descriptions.txt', train)\r\n","print('Descriptions train: =%d' % len(train_descriptions))\r\n","# prepare tokenizer\r\n","tokenizer = create_tokenizer(train_descriptions)\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print('Vocabulary Size: %d' % vocab_size)\r\n","# determine the maximum sequence length\r\n","max_length = max_length(train_descriptions)\r\n","print('Description Length: %d' % max_length)\r\n","\r\n","\r\n","# prepare test set\r\n","\r\n","# load test set\r\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.testImages.txt'\r\n","test = load_set(filename)\r\n","print('Testing Dataset: %d' % len(test))\r\n","# descriptions\r\n","test_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/descriptions.txt', test)\r\n","print('Descriptions test: =%d' % len(test_descriptions))\r\n","# photo features\r\n","test_features = load_photo_features('/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/features.pkl', test)\r\n","print('Photos test: =%d' % len(test_features))\r\n","\r\n","# load the model which has minimum loss, in this case it was model_1\r\n","models = 10\r\n","for i in range(models):\r\n","  # load the model which has minimum loss, in this case it was model_1\r\n","  print('BLEU Score of epoch ' + str(i+1) )\r\n","  filename = '/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/0.5/model' + str(i+1) + '.h5'\r\n","  model = load_model(filename)\r\n","  # evaluate model\r\n","  evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Dataset: 6000\n","Descriptions train: =6000\n","Vocabulary Size: 7579\n","Description Length: 34\n","Testing Dataset: 1000\n","Descriptions test: =1000\n","Photos test: =1000\n","BLEU Score of epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.481481\n","BLEU-2: 0.242810\n","BLEU-3: 0.141670\n","BLEU-4: 0.091046\n","BLEU Score of epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.518519\n","BLEU-2: 0.308607\n","BLEU-3: 0.208616\n","BLEU-4: 0.305038\n","BLEU Score of epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.464286\n","BLEU-2: 0.286239\n","BLEU-3: 0.195619\n","BLEU-4: 0.290529\n","BLEU Score of epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.519231\n","BLEU-2: 0.315321\n","BLEU-3: 0.214874\n","BLEU-4: 0.311946\n","BLEU Score of epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.438596\n","BLEU-2: 0.259762\n","BLEU-3: 0.182183\n","BLEU-4: 0.275282\n","BLEU Score of epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.473684\n","BLEU-2: 0.269953\n","BLEU-3: 0.186869\n","BLEU-4: 0.280630\n","BLEU Score of epoch 7\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-89d1d062bdc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BLEU Score of epoch '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m   \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/0.5/model'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m   \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/0.5/model7.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5imHFs6yVOZ","executionInfo":{"status":"ok","timestamp":1615894206462,"user_tz":-330,"elapsed":8193,"user":{"displayName":"Akash Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeGhNcXJOSVhbI3tqxY2POb52ehq8MIqOIRC0P=s64","userId":"06075092917726984214"}},"outputId":"19cca0cd-71ff-47ce-f4c2-ae768c62c417"},"source":["!pip install rouge\r\n","!pip install nltk==3.4.5\r\n","import nltk\r\n","nltk.download('wordnet')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: rouge in /usr/local/lib/python3.7/dist-packages (1.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n","Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (3.4.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5) (1.15.0)\n"],"name":"stdout"},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"TmtyLol_-HdS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b10a71b3-7802-46e0-e632-95254713b263"},"source":["\r\n","from numpy import argmax\r\n","from pickle import load\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.models import load_model\r\n","from nltk.translate.bleu_score import corpus_bleu\r\n","import nltk\r\n","from rouge import Rouge \r\n","\r\n","\r\n","# load doc into memory\r\n","def load_doc(filename):\r\n","\t# open the file as read only\r\n","\tfile = open(filename, 'r')\r\n","\t# read all text\r\n","\ttext = file.read()\r\n","\t# close the file\r\n","\tfile.close()\r\n","\treturn text\r\n","\r\n","# load a pre-defined list of photo identifiers\r\n","def load_set(filename):\r\n","\tdoc = load_doc(filename)\r\n","\tdataset = list()\r\n","\t# process line by line\r\n","\tfor line in doc.split('\\n'):\r\n","\t\t# skip empty lines\r\n","\t\tif len(line) < 1:\r\n","\t\t\tcontinue\r\n","\t\t# get the image identifier\r\n","\t\tidentifier = line.split('.')[0]\r\n","\t\tdataset.append(identifier)\r\n","\treturn set(dataset)\r\n","\r\n","# load clean descriptions into memory\r\n","def load_clean_descriptions(filename, dataset):\r\n","\t# load document\r\n","\tdoc = load_doc(filename)\r\n","\tdescriptions = dict()\r\n","\tfor line in doc.split('\\n'):\r\n","\t\t# split line by white space\r\n","\t\ttokens = line.split()\r\n","\t\t# split id from description\r\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\r\n","\t\t# skip images not in the set\r\n","\t\tif image_id in dataset:\r\n","\t\t\t# create list\r\n","\t\t\tif image_id not in descriptions:\r\n","\t\t\t\tdescriptions[image_id] = list()\r\n","\t\t\t# wrap description in tokens\r\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\r\n","\t\t\t# store\r\n","\t\t\tdescriptions[image_id].append(desc)\r\n","\treturn descriptions\r\n","\r\n","# load photo features\r\n","def load_photo_features(filename, dataset):\r\n","\t# load all features\r\n","\tall_features = load(open(filename, 'rb'))\r\n","\t# filter features\r\n","\tfeatures = {k: all_features[k] for k in dataset}\r\n","\treturn features\r\n","\r\n","# covert a dictionary of clean descriptions to a list of descriptions\r\n","def to_lines(descriptions):\r\n","\tall_desc = list()\r\n","\tfor key in descriptions.keys():\r\n","\t\t[all_desc.append(d) for d in descriptions[key]]\r\n","\treturn all_desc\r\n","\r\n","# fit a tokenizer given caption descriptions\r\n","def create_tokenizer(descriptions):\r\n","\tlines = to_lines(descriptions)\r\n","\ttokenizer = Tokenizer()\r\n","\ttokenizer.fit_on_texts(lines)\r\n","\treturn tokenizer\r\n","\r\n","\r\n","\r\n","# calculate the length of the description with the most words\r\n","def max_length(descriptions):\r\n","\tlines = to_lines(descriptions)\r\n","\treturn max(len(d.split()) for d in lines)\r\n","\r\n","# map an integer to a word\r\n","def word_for_id(integer, tokenizer):\r\n","\tfor word, index in tokenizer.word_index.items():\r\n","\t\tif index == integer:\r\n","\t\t\treturn word\r\n","\treturn None\r\n","\r\n","\r\n","\r\n","# generate a description for an image\r\n","def generate_desc(model, tokenizer, photo, max_length):\r\n","\t# seed the generation process\r\n","\tin_text = 'startseq'\r\n","\t# iterate over the whole length of the sequence\r\n","\tfor i in range(max_length):\r\n","\t\t# integer encode input sequence\r\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\r\n","\t\t# pad input\r\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\r\n","\t\t# predict next word\r\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\r\n","\t\t# convert probability to integer\r\n","\t\tyhat = argmax(yhat)\r\n","\t\t# map integer to word\r\n","\t\tword = word_for_id(yhat, tokenizer)\r\n","\t\t# stop if we cannot map the word\r\n","\t\tif word is None:\r\n","\t\t\tbreak\r\n","\t\t# append as input for generating the next word\r\n","\t\tin_text += ' ' + word\r\n","\t\t# stop if we predict the end of the sequence\r\n","\t\tif word == 'endseq':\r\n","\t\t\tbreak\r\n","\treturn in_text\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","def evaluate_ROUGE(model, descriptions, photos, tokenizer, max_length):\r\n","\tactual, predicted = list(), list()\r\n","\t# step over the whole set\r\n","\tfor key, desc in descriptions.items():\r\n","\t\t# generate description\r\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\r\n","\t\t# store actual and predicted\r\n","\t\t#references = [d.split() for d in desc]\r\n","\t\t#actual.join(references)\r\n","\t\tactual = desc\r\n","\t\tpredicted = yhat\r\n","\t\t#predicted.join(yhat)\r\n","\t\t\r\n","\t\t#print('Actual:    %s' %actual)\r\n","\t\t#print('Predected:  %s' %predicted)\r\n","\t\tif len(actual) >= 5:\r\n","\t\t\tbreak\r\n","\t\t\r\n","\t\t\r\n","\t# calculate ROUGE score\r\n","\trouge = Rouge()\r\n","\tscores = rouge.get_scores(predicted, \", \".join(actual), avg=True)\r\n","\tprint('ROUGE: ', scores)\r\n","\r\n","\r\n","def evaluate_METEOR(model, descriptions, photos, tokenizer, max_length):\r\n","\tactual, predicted = '',''\r\n","\t# step over the whole set\r\n","\t\r\n","\tfor key, desc in descriptions.items():\r\n","\t\t# generate description\r\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\r\n","\t\t# store actual and predicted\r\n","\t\t#references = [d.split() for d in desc]\r\n","\t\t#actual.join(references)\r\n","\t\tactual = desc\r\n","\t\tpredicted = yhat\r\n","\t\t#predicted.join(yhat)\r\n","\t\t\r\n","\t\t#print('Actual:    %s' %actual)\r\n","\t\t#print('Predected:  %s' %predicted)\r\n","\t\tif len(actual) >= 5:\r\n","\t\t\tbreak\r\n","\t\t\r\n","\t\t\r\n","\t# calculate METEOR score\r\n","\tprint('METEOR: %f' % nltk.translate.meteor_score.meteor_score(actual, predicted))\r\n","\t\t\r\n","\r\n","# prepare training set\r\n","\r\n","# load training dataset (6K)\r\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.trainImages.txt'\r\n","train = load_set(filename)\r\n","print('Training Dataset: %d' % len(train))\r\n","# descriptions\r\n","train_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/descriptions.txt', train)\r\n","print('Descriptions train: =%d' % len(train_descriptions))\r\n","# prepare tokenizer\r\n","tokenizer = create_tokenizer(train_descriptions)\r\n","vocab_size = len(tokenizer.word_index) + 1\r\n","print('Vocabulary Size: %d' % vocab_size)\r\n","# determine the maximum sequence length\r\n","max_length = max_length(train_descriptions)\r\n","print('Description Length: %d' % max_length)\r\n","\r\n","\r\n","# prepare test set\r\n","\r\n","# load test set\r\n","filename = '/content/drive/MyDrive/ICG/Flickr8k/Flickr_8k.testImages.txt'\r\n","test = load_set(filename)\r\n","print('Testing Dataset: %d' % len(test))\r\n","# descriptions\r\n","test_descriptions = load_clean_descriptions('/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/descriptions.txt', test)\r\n","print('Descriptions test: =%d' % len(test_descriptions))\r\n","# photo features\r\n","test_features = load_photo_features('/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/features.pkl', test)\r\n","print('Photos test: =%d' % len(test_features))\r\n","\r\n","# load the model which has minimum loss, in this case it was model_1\r\n","models = 10\r\n","for i in range(models):\r\n","  # load the model which has minimum loss, in this case it was model_1\r\n","  filename = '/content/drive/MyDrive/ICG/VGG16_with_BidirectionalLSTM/0.2/model' + str(i+1) + '.h5'\r\n","  model = load_model(filename)\r\n","  # evaluate model\r\n","  print('METEOR Score of epoch ' + str(i+1) )\r\n","  evaluate_METEOR(model, test_descriptions, test_features, tokenizer, max_length)\r\n","  print('ROUGE Score of epoch ' + str(i+1) )\r\n","  evaluate_ROUGE(model, test_descriptions, test_features, tokenizer, max_length)\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Dataset: 6000\n","Descriptions train: =6000\n","Vocabulary Size: 7579\n","Description Length: 34\n","Testing Dataset: 1000\n","Descriptions test: =1000\n","Photos test: =1000\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["METEOR Score of epoch 1\n","METEOR: 0.506051\n","ROUGE Score of epoch 1\n","ROUGE:  {'rouge-1': {'f': 0.2025316431725685, 'p': 0.7272727272727273, 'r': 0.11764705882352941}, 'rouge-2': {'f': 0.05194804968797446, 'p': 0.2, 'r': 0.029850746268656716}, 'rouge-l': {'f': 0.3076923043565089, 'p': 0.7272727272727273, 'r': 0.1951219512195122}}\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["METEOR Score of epoch 2\n","METEOR: 0.516274\n","ROUGE Score of epoch 2\n","ROUGE:  {'rouge-1': {'f': 0.1298701278057008, 'p': 0.5555555555555556, 'r': 0.07352941176470588}, 'rouge-2': {'f': 0.05333333142755563, 'p': 0.25, 'r': 0.029850746268656716}, 'rouge-l': {'f': 0.19999999704800003, 'p': 0.5555555555555556, 'r': 0.12195121951219512}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZUxFKl3wxlze"},"source":[""],"execution_count":null,"outputs":[]}]}